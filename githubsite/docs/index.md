LLMs don't behave in a predictable way.  They change with every model training and alignment update.  There are no industry standard practices for evaluating them.

Contrast this to the traditional software development cycle in which every software release can be evaluated against precise testing scenarios and the developer or quality assurance specialist can have complete confidence in how well the system performed against those tests.

The LLM evaluation is a new area of exploration.  Every LLM evaluation must take into account the non-deterministic nature of LLMs which can have different outputs even when the starting inputs are the same, making it unpredictable.  An evaluation must be able to manage a degree of variation among 'correct' responses.  This challenge is multiplied when assessing LLMs across different task types and domains of knowledge.

This website is a repository for my knowledge of the field, as I learn it.

If I test a new framework, I'll share my impressions here.

If I see a relevant academic paper or article, I'll write up my notes here.