LLMs don't behave in a predictable way.  They change with every model training and alignment update.  There are no industry standard practices for evaluating them.

In the typical software development cycle, this is very different.  When traditional software is released or updated, a developer or quality assurance specialist can write tests against precise use cases and have complete confidence in how well the system performed against those tests.

The behavior of an LLM is much more difficult to evaluate.  Any evaluation needs to take into account the probabilistic nature of LLMs and the resulting large variation of possible 'correct' responses.  This challenge is multiplied when assessing LLMs across a number of different tasks and domains of knowledge.

This is a repository for my knowledge of the field, as I learn it.

If I test a new framework, I'll share my impressions here.

If I see a relevant academic paper or article, I'll write up my notes here.