Evaluation of LLMs is really hard.  They don't behave in a predictable way, by definition.  They change with every model training and alignment update.  There are no industry standard practices for evaluating them.  And the evaluation process can be quite different depending on the vertical domain of knowledge.

In the past, when some new software was released or updated, a developer or QA specialist could write tests against precise use cases, and have complete confidence in how well the system performed against those tests.

Today, when a non-deterministic LLM is released or updated, the behavior is much more difficult to evaluate.  Any evaluation needs to take into account the probabalistic nature of the reponses and incorporate a number of tasks and domains.   And there are fundamental questions that no one has agreed upon yet, like whether the prompts need to stay the same between evaluations.

If I test a new framework, I'll drop my raw notes and findings in here.

If I see a relevant academic paper, I'll read it and write my notes here.