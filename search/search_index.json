{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LLMs don't behave in a predictable way. They change with every model training and alignment update. There are no industry standard practices for evaluating them. Contrast this to the traditional software development cycle in which every software release can be evaluated against precise testing scenarios and the developer or quality assurance specialist can have complete confidence in how well the system performed against those tests. The LLM evaluation is a new area of exploration. Every LLM evaluation must take into account the non-deterministic nature of LLMs which can have different outputs even when the starting inputs are the same, making it unpredictable. An evaluation must be able to manage a degree of variation among 'correct' responses. This challenge is multiplied when assessing LLMs across different task types and domains of knowledge. This website is a repository for my knowledge of the field, as I learn it. If I test a new framework, I'll share my impressions here. If I see a relevant academic paper or article, I'll write up my notes here.","title":"Home"},{"location":"AI-Evaluation-Tools-and-Libraries/","text":"Here is my list of LLM evaluation frameworks. It's a work in progress... Inspect https://ukgovernmentbeis.github.io/inspect_ai/ Giskard https://github.com/Giskard-AI/giskard When you run 'scan' with Giskard, it uses built-in detectors. Most of the detectors use GPT-4, including these vulnerability detectors: o Hallucination and Misinformation o Harmful Content Generation o Prompt Injection o Robustness o Output Formatting o Information Disclosure o Stereotypes and Discrimination ChainForge Website EvalGen Paper Includes web server with GUI for evaluation flows. Can run as local server or use hosted version. Galileo (requires sales approval to test) They create their own models. Evaluate : Rapid Evaluation of Prompts, Chains and RAG systems Observe : Real-time Observability for GenAI Apps and Models Protect : Real-time Request and Response Interception Arthur Bench Website Arize - Phoenix Open Source Library Website BrainTrust Data - HoneyHive - Startup based in New York. - Patronus - Startup backed by Lightspeed. - PromptLayer - New York based startup. Visual pipeline builder for evaluations. LangSmith Spinoff licensed software from Langchain open source. Humanloop YC-based startup. Recently moved to San Francisco. EleutherAI LM Evaluation Harness GitHub Repository List of 60 Evaluations AI2 WildBench Website We carefully curate a collection of 1024 hard tasks from real users, which cover common use cases such as code debugging, creative writing, and data analysis. Portal SuperPipe Website Background Ragas Documentation Framework for RAG that synthesizes question/answer test data sets based on RAG documents. OpenAI Evals Library GitHub Repository Long Tail of Related Libraries - String2String : Open source suite of new and traditional algorithms for string comparison, from Stanford - Instructor https://github.com/jxnl/instructor - Fructose : GitHub Repository by the founder of banana.dev - Infini-Gram : Website - BERT Score : GitHub Repository","title":"AI Evaluation Tools and Libraries"},{"location":"BERTscore%20and%20friends/","text":"June 2nd, 2024 A quick project to learn something new... I had come across the use of Bert Score for LLM evaluation in a few different places and got curious enough to set up my own little eval. I looked for someone hosting an API for Bert Score so I didn't need to run the whole model locally, but ended up installing PyTorch and found that it actually runs very quick on an Macbook Air M2. The whole setup consisted of a python script and Postgres DB to store my prompts and gold answers and keep a log of the scores. I did a query to OpenAI for each prompt and scored with 3 different measures: - Simple word intersection in Python - Bert Score using the bert_score library - LLM as a judge (also using OpenAI) The prompts are my own hand-crafted set of questions on topics that I know well. I plan to build this dataset out quite a bit in future. I'm finding that using my own data sets is a really intuitive way to get to learn about different LLM evaluation methodologies. Next up, I want to run some tests with Infini-gram and Named Entity Recognition (NER).","title":"BERTscore and friends"},{"location":"Research%20Paper%20on%20Healthcare%20Bias%20in%20The%20Pile/","text":"June 2nd, 2024 A recent research paper demonstrated that healthcare bias exists within a commonly-used dataset for training LLMs. The Pile data set The data set they examined is called 'The Pile' and consists of 825 GB of English text from 22 data sources. Bias in The Pile The findings show that the dataset contains bias in that the disease prevalence for different demographic groups in the data does not match actual disease prevalences according to academic research and epidemiological data. Bias in AI models created with The Pile They looked at the Mamba and Pythia models, which are strictly pre-trained on The Pile only. They looked at the Llama2, Llama3, Mistral, and Qwen1.5 models and found the models had associations for prevalence in different languages (Chinese, English, French, and Spanish). The methodology included a ranking method to analyzes disease subgroups based on their co-occurrences in The Pile as well as the \u201cgold\" data set derived from real-world data. This empirical method bypasses model outputs, directly measuring disease representation across different demographic contexts. As part of the comparison, they used the Infini-gram public API for examining n-grams of any length. The study points to the urgent need for better healthcare data, with actual disease prevalences, to be incorporated into commonly used training data sets. \"Efforts will be made to create and employ datasets that furnish more accurate and exhaustive real-world prevalence data for diseases, especially those poorly represented in existing datasets.\" Read the paper: Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias https://arxiv.org/pdf/2405.05506 [https://arxiv.org/pdf/2405.05506]","title":"Research Paper on Healthcare Bias in The Pile"},{"location":"Research%20Paper%20on%20Healthcare%20Bias%20in%20The%20Pile/#the-pile-data-set","text":"The data set they examined is called 'The Pile' and consists of 825 GB of English text from 22 data sources.","title":"The Pile data set"},{"location":"Research%20Paper%20on%20Healthcare%20Bias%20in%20The%20Pile/#bias-in-the-pile","text":"The findings show that the dataset contains bias in that the disease prevalence for different demographic groups in the data does not match actual disease prevalences according to academic research and epidemiological data.","title":"Bias in The Pile"},{"location":"Research%20Paper%20on%20Healthcare%20Bias%20in%20The%20Pile/#bias-in-ai-models-created-with-the-pile","text":"They looked at the Mamba and Pythia models, which are strictly pre-trained on The Pile only. They looked at the Llama2, Llama3, Mistral, and Qwen1.5 models and found the models had associations for prevalence in different languages (Chinese, English, French, and Spanish). The methodology included a ranking method to analyzes disease subgroups based on their co-occurrences in The Pile as well as the \u201cgold\" data set derived from real-world data. This empirical method bypasses model outputs, directly measuring disease representation across different demographic contexts. As part of the comparison, they used the Infini-gram public API for examining n-grams of any length. The study points to the urgent need for better healthcare data, with actual disease prevalences, to be incorporated into commonly used training data sets. \"Efforts will be made to create and employ datasets that furnish more accurate and exhaustive real-world prevalence data for diseases, especially those poorly represented in existing datasets.\" Read the paper: Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias https://arxiv.org/pdf/2405.05506 [https://arxiv.org/pdf/2405.05506]","title":"Bias in AI models created with The Pile"},{"location":"evaluation-by-Jason-Wei/","text":"June 2nd, 2024 I recently read this very insightful blog post by Jason Wei who is an AI researcher at OpenAI. Key points from Jason Wei on LLM evaluation The evaluation is the benchmarking data set. He previously created two benchmarking data sets that became popular, MGSM and BBH. For an eval to become popular, it needs to mean something 'significant and easily understandable'. A good eval data set should have lots of examples of high quality, but not be too complicated with multiple metrics and subsets. It shouldn't require complicated infrastructure either. It should measure something that is central to intelligence. It should be hard enough that it shows relative performance, even while models get more capable. LLMs have made evaluations substantially harder with multiple tasks and long responses. There is currently no single eval that adequately evaluates LLMs. Using human pairing with models, LMSYS, creates subjectivity problems and feel and style can be weighted more heavily than accuracy. Model-generated synthetic data sets are becoming more popular for evaluations, and are useful for quick experiments, but it takes a lot of work to create a data set good enough to stand the test of time. High quality domain specific eval data sets, e.g. medical and legal, are important and valuable but by their nature will never attain the popularity of a more general test of intelligence. Contamination is becoming a big issue. Eval questions will get shared on the internet. The alternative is to keep the test set hidden, but that adds complications. The AI research community should invest more in evals, even though it is not rewarded as much as modeling work. Read the full article at [https://www.jasonwei.net/blog/evals].","title":"Evals by Jason Wei"},{"location":"evaluation-by-Jason-Wei/#key-points-from-jason-wei-on-llm-evaluation","text":"The evaluation is the benchmarking data set. He previously created two benchmarking data sets that became popular, MGSM and BBH. For an eval to become popular, it needs to mean something 'significant and easily understandable'. A good eval data set should have lots of examples of high quality, but not be too complicated with multiple metrics and subsets. It shouldn't require complicated infrastructure either. It should measure something that is central to intelligence. It should be hard enough that it shows relative performance, even while models get more capable. LLMs have made evaluations substantially harder with multiple tasks and long responses. There is currently no single eval that adequately evaluates LLMs. Using human pairing with models, LMSYS, creates subjectivity problems and feel and style can be weighted more heavily than accuracy. Model-generated synthetic data sets are becoming more popular for evaluations, and are useful for quick experiments, but it takes a lot of work to create a data set good enough to stand the test of time. High quality domain specific eval data sets, e.g. medical and legal, are important and valuable but by their nature will never attain the popularity of a more general test of intelligence. Contamination is becoming a big issue. Eval questions will get shared on the internet. The alternative is to keep the test set hidden, but that adds complications. The AI research community should invest more in evals, even though it is not rewarded as much as modeling work. Read the full article at [https://www.jasonwei.net/blog/evals].","title":"Key points from Jason Wei on LLM evaluation"},{"location":"evaluation-principles-by-Clementine/","text":"May 24th, 2024 Today I read a great essay by Cl\u00e9mentine Fourrier who is responsible for LLM evaluation and the leaderboard at Huggingface. He breaks the field of LLM evaluations into three types: automated benchmarking, humans as judges, and LLM models as judges. Automated benchmarking Automated benchmarking requires two things: a list of gold standard samples, a metric for scoring. Some things are hard to measure with automated benchmarks, for example high school math with the GSM8K data set or \"is this model good at writing poetry?\". A big problem is filtering out benchmarketing data that gets into the training set. Some techniques include: adding a \"canary string\" of characters for people to look for and remove from training sets, providing benchmarks in an encrypted form or gating system. Contamination is when the evaluation dataset ends up in the training set. Using dynamic benchmarks can address this but that's expensive. Human as a judge This is done by tasking humans with prompting a model and then grading the model answer. Approaches include: The 'vibes-check' which is someone in the LLM community getting a feeling for how the model performs by crafting their own prompts across a range of areas. The 'arena' where users chat with multiple models and vote on the best answer. The 'systematic annotations' approach involves paid annotators and is the most methodical. It's expensive and there can be a lot of biases depending on who is evaluating, the assertiveness of the model in their response, the tone of the response, etc. Model as a judge This involves letting the LLM be the judge. Sometimes big and capable models are used, while other times smaller and more specially trained models are put to the task. Some shortcomings of LLM as a judge is that they tend to prefer their own responses to other models, they are bad at numerical scoring, and their opinions often doesn't align with human judges. There is also the risk that when your entire generation and feedback loop consists only of LLMs, small detrimental changes might get baked into the system and won't be apparent for a long time. Why evaluations are done: 1) Regression testing. Make sure the software didn't break or get worse. 2) Determine which model is the best in the rankings. 3) Determine whether a model can perform a specific capability. Read the full essay at https://huggingface.co/blog/clefourrier/llm-evaluation .","title":"Evals by Clementine"},{"location":"evaluation-principles-by-Clementine/#automated-benchmarking","text":"Automated benchmarking requires two things: a list of gold standard samples, a metric for scoring. Some things are hard to measure with automated benchmarks, for example high school math with the GSM8K data set or \"is this model good at writing poetry?\". A big problem is filtering out benchmarketing data that gets into the training set. Some techniques include: adding a \"canary string\" of characters for people to look for and remove from training sets, providing benchmarks in an encrypted form or gating system. Contamination is when the evaluation dataset ends up in the training set. Using dynamic benchmarks can address this but that's expensive.","title":"Automated benchmarking"},{"location":"evaluation-principles-by-Clementine/#human-as-a-judge","text":"This is done by tasking humans with prompting a model and then grading the model answer. Approaches include: The 'vibes-check' which is someone in the LLM community getting a feeling for how the model performs by crafting their own prompts across a range of areas. The 'arena' where users chat with multiple models and vote on the best answer. The 'systematic annotations' approach involves paid annotators and is the most methodical. It's expensive and there can be a lot of biases depending on who is evaluating, the assertiveness of the model in their response, the tone of the response, etc.","title":"Human as a judge"},{"location":"evaluation-principles-by-Clementine/#model-as-a-judge","text":"This involves letting the LLM be the judge. Sometimes big and capable models are used, while other times smaller and more specially trained models are put to the task. Some shortcomings of LLM as a judge is that they tend to prefer their own responses to other models, they are bad at numerical scoring, and their opinions often doesn't align with human judges. There is also the risk that when your entire generation and feedback loop consists only of LLMs, small detrimental changes might get baked into the system and won't be apparent for a long time. Why evaluations are done: 1) Regression testing. Make sure the software didn't break or get worse. 2) Determine which model is the best in the rankings. 3) Determine whether a model can perform a specific capability. Read the full essay at https://huggingface.co/blog/clefourrier/llm-evaluation .","title":"Model as a judge"},{"location":"llm-evaluations/","text":"February 28th, 2024 One of the hallmarks of a new technology reaching the mainstream is when it faces its first landmark lawsuit. In the case of LLMs, it was a case of Air Canada inadequately testing their chatbot. A customer of Air Canada successfully sued the airline for an overcharge after receiving bad advice about a ticket purchase. The impact of this court ruling is making companies think more deeply about the risks of AI services. The reality is that no one has prepared for this moment. The evaluation of large language models (LLMs) is a problem that has not yet been solved. It has not yet been decided who is responsible for the quality of the output of a Large Language Model (LLM). Many would claim the foundational AI model providers like OpenAI should be responsible, while others would point to the companies who integrate LLM technology into their consumer or business offerings. Meanwhile, the regulatory landscape is still in its infancy. Software development of an LLM application is quite different from anything that has come before. In the initial steps of application building, the development process might be deceptively easy. For instance, when creating a simple chatbot, very little programming is required since the LLM accepts human language as the input. Add a simple basic website template and a few API calls and a fully functional chatbot can be created in an hour or so. But the creators of this LLM application will have no easy way to determine if it's good enough to become customer-facing. Today's state of the art LLMs do not come packaged with any standard testing method or automated evaluation. There are no industry protocols and few guidelines. It's new territory for even the most experienced developers. The reason is that LLMs are non-deterministic. The variation in these responses does not lend itself to an easy quantified assessment. An LLM may provide two different responses if asked the exact same question two times. And its responses may change and evolve over time. Even worse, if the LLM vendor makes a scheduled release, or does an unannounced 'internal alignment', the responses for entire categories of prompts can change significantly. The good news is that there are a growing number of innovative approaches for addressing the hard parts of LLM evaluation, a few of which are outlined below. It might seem obvious, but in order to do an LLM evaluation, it's necessary to have a good data set for testing. For those who specialize in machine learning (ML) and data science, it's standard practice to start with the process of obtaining the data, preparing the data, and splitting the data into training and test sets. For non-ML engineers, this process might be part of the learning curve. One technique to get started quickly is to create a small hand-crafted data set with manual-labels. With this small data set in hand, an evaluation pipeline can be set up quickly. While this initial evaluation data set might not be large enough to be meaningful, it facilitates the setup of an evaluation pipeline. It also encourages a habit of investigating individual evaluation findings in detail. As a further step, there are many open source ML data sets that can be useful for a more general evaluation of LLM applications. For example, the Eleuther evaluation framework includes over 60 standard benchmarks for LLMs with hundreds of subtasks and variants. There are datasets that focus on logical reasoning, computer programming, academic examinations, and identifying bias or inappropriate behavior. If the LLM application is intended for use in a specialized domain (law, medical, internal corporate, etc), obtaining a domain specific data set for evaluation is crucial but can be challenging. For example, in medicine there are a number of open data sets that can be freely downloaded. There are others that require a certificate demonstrating knowledge of patient privacy before they can be obtained. But most of the data is in fact locked inside various institutional entities and can only be acquired by a very long process of signing an agreement with a hospital or healthcare company. Synthetic data for evaluation Once a data set is in hand, it's often useful to make it larger and more varied by a process of generating synthetic data. For example, words in the data can be replaced by variables which results in many variations of prompts generated from the same data set. To create more complex synthetic data sets, an increasingly popular approach is to use an LLM that is external to the application to generate the data. Synthetic data sets can be created for purpose of creating sets of question answer pairs, multiple variations of prompts, assessing the impact of text changes like verb tenses and misspellings, or system changes like adjusting the temperature or randomness parameter of the LLM response. Orchestration In the LLM application workflow, there are typically a number of different steps, each of which can be logged and evaluated separately. For the logging itself, there are a number of companies and open source projects that specialize in AI tracing and observability of steps in the application workflow. An example of a workflow would be if an application uses Retrieval Augmented Generation (RAG) to retrieve text from a vector database with embeddings from internal documents, an evaluation step can be inserted after the text retrieval to provide a score of the match quality. This score can be used on its own or in combination with other metrics for an aggregated quality score. Scoring In the world of LLMs, there is not always a correct answer. In many cases the ground truth for a given question may have a large degree of interpretation. While sometimes the answer is binary or multiple choice, other times there may be multiple correct answers with different words, phrasings, and shades of meaning. Options for scoring these text responses include algorithmic text comparison, calculations of embedding distance, and prompting external LLMs to be a judge of the response. If using an LLM as a judge, be mindful of the fact that LLMs are not very good at numerical scoring. Human testing Due to the challenges of automated LLM evaluation, it's standard practice to design any evaluation with a human in the loop. The human may be there only to spot check data or the human may be judging responses and generating quality scores. Alignment and Bias There have been many academic papers demonstrating problematic behavior by LLM foundational models. These models are only as good as the data they have been trained on, which includes a lot of questionable content as well as content that represents existing inequalities in society. While the LLMs models have generally been fine-tuned to improve their behavior and prevent illegal and offenses responses, it's still often necessary to do further evaluation of quality. There are open source data sets and commercial tools to assist with general alignment evaluation. But for any specific domain, it's important to involve the people who have the deepest knowledge of that domain to validate that the system is not doing something bad. Final thoughts Don't put prototype LLM applications directly into production with customers. Instead, spend some time evaluating it. Future There are a number of software packages available to assist with AI evaluation, including open source libraries, new startup products, and offerings from big tech. That's a topic for another writing session.","title":"Basics of LLM Evaluations"},{"location":"llm-evaluations/#synthetic-data-for-evaluation","text":"Once a data set is in hand, it's often useful to make it larger and more varied by a process of generating synthetic data. For example, words in the data can be replaced by variables which results in many variations of prompts generated from the same data set. To create more complex synthetic data sets, an increasingly popular approach is to use an LLM that is external to the application to generate the data. Synthetic data sets can be created for purpose of creating sets of question answer pairs, multiple variations of prompts, assessing the impact of text changes like verb tenses and misspellings, or system changes like adjusting the temperature or randomness parameter of the LLM response.","title":"Synthetic data for evaluation"},{"location":"llm-evaluations/#orchestration","text":"In the LLM application workflow, there are typically a number of different steps, each of which can be logged and evaluated separately. For the logging itself, there are a number of companies and open source projects that specialize in AI tracing and observability of steps in the application workflow. An example of a workflow would be if an application uses Retrieval Augmented Generation (RAG) to retrieve text from a vector database with embeddings from internal documents, an evaluation step can be inserted after the text retrieval to provide a score of the match quality. This score can be used on its own or in combination with other metrics for an aggregated quality score.","title":"Orchestration"},{"location":"llm-evaluations/#scoring","text":"In the world of LLMs, there is not always a correct answer. In many cases the ground truth for a given question may have a large degree of interpretation. While sometimes the answer is binary or multiple choice, other times there may be multiple correct answers with different words, phrasings, and shades of meaning. Options for scoring these text responses include algorithmic text comparison, calculations of embedding distance, and prompting external LLMs to be a judge of the response. If using an LLM as a judge, be mindful of the fact that LLMs are not very good at numerical scoring.","title":"Scoring"},{"location":"llm-evaluations/#human-testing","text":"Due to the challenges of automated LLM evaluation, it's standard practice to design any evaluation with a human in the loop. The human may be there only to spot check data or the human may be judging responses and generating quality scores.","title":"Human testing"},{"location":"llm-evaluations/#alignment-and-bias","text":"There have been many academic papers demonstrating problematic behavior by LLM foundational models. These models are only as good as the data they have been trained on, which includes a lot of questionable content as well as content that represents existing inequalities in society. While the LLMs models have generally been fine-tuned to improve their behavior and prevent illegal and offenses responses, it's still often necessary to do further evaluation of quality. There are open source data sets and commercial tools to assist with general alignment evaluation. But for any specific domain, it's important to involve the people who have the deepest knowledge of that domain to validate that the system is not doing something bad.","title":"Alignment and Bias"},{"location":"llm-evaluations/#final-thoughts","text":"Don't put prototype LLM applications directly into production with customers. Instead, spend some time evaluating it.","title":"Final thoughts"},{"location":"llm-evaluations/#future","text":"There are a number of software packages available to assist with AI evaluation, including open source libraries, new startup products, and offerings from big tech. That's a topic for another writing session.","title":"Future"}]}