# evals-and-inference
For a while now, I've been wanting to share more online. I love the model of learning in public. My aspiration is to post openly about the thought processes as well as any knowledge that I might learn.

The focus of my writing is external evalauation of LLMs as black box systems without access to the model architecture. The scope includes exploring evaluation frameworks, identifying quality testing data sets, synthesizing data, middle layers like RAG and prompt optimization, and performance metrics from accuracy and consistency to bias and misalignment.  I'll also write about evaluation for specific domains, especially medical AI which has been a focus of mine.  

If I test a new framework, I'll drop my raw notes and findings in here.

If I see a relevant academic paper, I'll read it and write my notes here. 

I'll regularly review those raw notes and write summaries.

I'll try to publish both as longer-form blog posts and articles and short summaries on social media.  Maybe some videos too.

My 'getting started' methodology for this repo is the following:
*  Start small and private
*  Set a daily cadence with calendar reminders for self-accountability
*  Set a schedule for measuring my progress and recognizing improvement
*  Find external accountability partners to stay on track
*  Distribute only to a private audience at first, then expand to the public sphere
