# evals-and-inference
For a while now, I've been wanting to post more of my knowledge online. I love the model of learning in public model.  My aspiration is to post widely so others can learn from my wrong turns as well as my more significant findings.

My topic is AI LLM evaluations including different evaluation frameworks, identifying quality data sets for benchmarks and synthesizing data, and exploring middle layers like RAG and prompt optimization that influence the evaluation.

If I test a new framework, I'll drop my raw notes and findings in here.

If I see a relevant academic paper, I'll read it and write my notes here. 

I'll regularly review those raw notes and write summaries.

I'll try to publish both as longer-form blog posts and articles and short summaries on social media.  Maybe some video tutorials too.

My 'getting started' methodology for this repo is the following:
*  Start small and private
*  Set a daily cadence with calendar reminders for self-accountability
*  Set a schedule for measuring my progress and recognizing improvement
*  Find external accountability partners to stay on track
*  Distribute only to a private audience at first, then expand to the public sphere
